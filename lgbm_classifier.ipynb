{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df1 is (100828, 5)\n",
      "Shape of df2 is (7567, 5)\n"
     ]
    }
   ],
   "source": [
    "from lib import *\n",
    "from preprocess import *\n",
    "from report import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "notebook_path = os.path.abspath(\"lgbm_classifier.ipynb\"+ '/..')\n",
    "\n",
    "# Constants\n",
    "random_state = 23\n",
    "n_splits = 9\n",
    "test_size = 0.2\n",
    "n_round = 4\n",
    "\n",
    "# Read data\n",
    "df1 = pd.read_csv('data/df1.csv', delimiter=',')\n",
    "df2 = pd.read_csv('data/df2.csv', delimiter=',')\n",
    "\n",
    "print(f'Shape of df1 is {df1.shape}')\n",
    "print(f'Shape of df2 is {df2.shape}')\n",
    "\n",
    "type_graph = ['distplot', 'hist']\n",
    "features = ['value', 'duration']\n",
    "cat_feature = 'event_type'\n",
    "\n",
    "\n",
    "bootstrap_bot = bootstrap(n=df1.shape[0], arr=df2, cols=features, cat_col=cat_feature)\n",
    "\n",
    "df = Preparation(df1=df1, df2=bootstrap_bot, features=features, cat_feature=cat_feature)\n",
    "\n",
    "X, y = df.create_df()\n",
    "spl = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Split on train test and scale data\n",
    "X_train_pre, X_test_pre, y_train, y_test = df.prep_split_data()\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    cv_scores =[]\n",
    "    \n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [100, 500, 1000]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.001, 0.01, 0.1]),\n",
    "        \"num_leaves\": trial.suggest_categorical(\"num_leaves\", [3, 5, 10]),\n",
    "        \"max_depth\": trial.suggest_categorical(\"max_depth\", [3, 7]),\n",
    "        \"min_split_gain\": trial.suggest_categorical(\"min_split_gain\", [0, 0.5]),\n",
    "        \"min_child_samples\": trial.suggest_categorical(\"min_child_samples\", [1, 5]),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n",
    "        \"reg_alpha\": trial.suggest_categorical(\"reg_alpha\", [0, 1, 10]),\n",
    "        \"reg_lambda\": trial.suggest_categorical(\"reg_lambda\", [0, 1, 10])}\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    model = lgb.LGBMClassifier(objective=\"binary\", \n",
    "                               class_weight='balanced', \n",
    "                               random_state=23, \n",
    "                               subsample=1.0, \n",
    "                               subsample_freq=10, \n",
    "                               colsample_bytree=0.85,\n",
    "                               **param_grid)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              eval_set = [(X_test, y_test)],\n",
    "              eval_metric=\"binary_logloss\",\n",
    "              early_stopping_rounds=10,\n",
    "              verbose=-1,\n",
    "              callbacks=[LightGBMPruningCallback(trial, metric=\"binary_logloss\")])\n",
    "        \n",
    "    preds = model.predict_proba(X_test)\n",
    "    cv_scores.append(log_loss(y_test, preds))\n",
    "\n",
    "    return cv_scores\n",
    "\n",
    "#study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "##func = lambda trial: objective(trial, X_train_pre, y_train)\n",
    "\n",
    "#n_trials = 100\n",
    "\n",
    "#optim = study.optimize(func, n_trials=n_trials)\n",
    "#print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "#print(f\"\\tBest params:\")\n",
    "\n",
    "#for key, value in study.best_params.items():\n",
    "    #print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "best_params = {'n_estimators': 1000,\n",
    "               'learning_rate': 0.01,\n",
    "               'max_depth': 5,\n",
    "               'boosting_type': 'gbdt'}\n",
    "\n",
    "lgb_best = lgb.LGBMClassifier(objective=\"binary\",\n",
    "                          class_weight='balanced', \n",
    "                          random_state=random_state,\n",
    "                          **best_params)\n",
    "\n",
    "lgb_best_fit = lgb_best.fit(X_train_pre,\n",
    "                            y_train,\n",
    "                            eval_set = [(X_train_pre, y_train), (X_test_pre, y_test)],\n",
    "                            eval_metric=\"binary_logloss\",\n",
    "                            verbose=-1,\n",
    "                            early_stopping_rounds=10)\n",
    "\n",
    "y_pred_proba_test = lgb_best_fit.predict(X_test_pre)\n",
    "y_pred_proba_train = lgb_best_fit.predict(X_train_pre)\n",
    "\n",
    "report = Report(y_train=y_train, y_test=y_test, y_pred_train=y_pred_proba_train, y_pred_test=y_pred_proba_test)\n",
    "classification_report= report.classification_report(name='lgbm_report')\n",
    "report.test_overfitting(n_round=n_round)\n",
    "overfit_logreg = report.plot_overfitting(model='lgbm', save=True, name_fig='overfitting')\n",
    "roc_pr = report.roc_auc_pr_plot(name_fig='lgbm_ROC-PR')\n",
    "\n",
    "lgb.plot_metric(lgb_best_fit)\n",
    "plt.savefig(notebook_path + '/graph/' + 'lgbm_metric_plot.png', dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
